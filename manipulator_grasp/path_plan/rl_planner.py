"""
RL Path Planner Inference Wrapper
RLè·¯å¾„è§„åˆ’å™¨æ¨ç†å°è£…

æä¾›è®­ç»ƒå¥½çš„RLç­–ç•¥çš„åŠ è½½å’Œä½¿ç”¨æ¥å£,
ç”Ÿæˆä»èµ·ç‚¹åˆ°ç›®æ ‡çš„è½¨è¿¹
"""

import os
import sys
import numpy as np
from typing import Optional, Tuple, Dict, Any
from enum import Enum

import pinocchio
import mujoco

# æ·»åŠ é¡¹ç›®è·¯å¾„
ROOT_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, ROOT_DIR)


class PlannerType(Enum):
    """è·¯å¾„è§„åˆ’å™¨ç±»å‹"""
    RRT_CONNECT = "rrt_connect"
    RL_POLICY = "rl_policy"
    HYBRID = "hybrid"  # å…ˆå°è¯•RL,å¤±è´¥åˆ™å›é€€åˆ°RRT


class RLPlanner:
    """
    RLç­–ç•¥å°è£…ç±»
    
    åŠ è½½è®­ç»ƒå¥½çš„PPOç­–ç•¥å¹¶ç”¨äºç”Ÿæˆè½¨è¿¹
    """
    
    def __init__(
        self,
        model_path: str,
        pin_model: Any = None,
        pin_data: Any = None,
        mj_model: Any = None,
        mj_data: Any = None,
        max_steps: int = 500,
        dt: float = 0.02,
    ):
        """
        åˆå§‹åŒ–RLè§„åˆ’å™¨
        
        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„ (.zipæ–‡ä»¶)
            pin_model: Pinocchioæ¨¡å‹ (ç”¨äºæ­£å‘è¿åŠ¨å­¦)
            pin_data: Pinocchioæ•°æ®
            mj_model: MuJoCoæ¨¡å‹ (ç”¨äºç¢°æ’æ£€æµ‹)
            mj_data: MuJoCoæ•°æ®
            max_steps: æœ€å¤§æ­¥æ•°
            dt: æ—¶é—´æ­¥é•¿
        """
        self.model_path = model_path
        self.pin_model = pin_model
        self.pin_data = pin_data
        self.mj_model = mj_model
        self.mj_data = mj_data
        self.max_steps = max_steps
        self.dt = dt
        
        self.policy = None
        self.n_joints = 6
        self.max_joint_velocity = 1.0
        self.success_threshold = 0.2  # Must match RLPathEnv
        
        # å…³èŠ‚é™åˆ¶
        self.joint_limits_low = np.array([-2*np.pi, -2*np.pi, -np.pi, -2*np.pi, -2*np.pi, -2*np.pi])
        self.joint_limits_high = np.array([2*np.pi, 2*np.pi, np.pi, 2*np.pi, 2*np.pi, 2*np.pi])
        
        # åŠ è½½æ¨¡å‹
        self._load_model()
        
    def _load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        try:
            from stable_baselines3 import PPO
            
            if os.path.exists(self.model_path):
                self.policy = PPO.load(self.model_path)
                print(f"âœ… RL policy loaded from: {self.model_path}")
            else:
                print(f"âš ï¸ Model not found: {self.model_path}")
                self.policy = None
        except ImportError:
            print("âš ï¸ stable_baselines3 not installed. RL planner will not work.")
            self.policy = None
        except Exception as e:
            print(f"âŒ Failed to load RL model: {e}")
            self.policy = None
            
    def is_ready(self) -> bool:
        """æ£€æŸ¥ç­–ç•¥æ˜¯å¦å·²åŠ è½½"""
        return self.policy is not None
        
    def _get_ee_position(self, q: np.ndarray) -> np.ndarray:
        """è®¡ç®—æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®"""
        if self.pin_model is not None and self.pin_data is not None:
            q_full = np.zeros(self.pin_model.nq)
            q_full[:6] = q
            pinocchio.forwardKinematics(self.pin_model, self.pin_data, q_full)
            pinocchio.updateFramePlacements(self.pin_model, self.pin_data)
            frame_id = self.pin_model.getFrameId("grasp_center")
            return self.pin_data.oMf[frame_id].translation.copy()
        return np.zeros(3)
        
    def _check_collision(self, q: np.ndarray) -> bool:
        """
        æ£€æŸ¥æœºå™¨äººæ˜¯å¦å‘ç”Ÿç¢°æ’
        
        åªæ£€æµ‹æ¶‰åŠæœºå™¨äººlinkçš„ç¢°æ’,å¿½ç•¥:
        - åœºæ™¯ä¸­ç‰©ä½“ä¹‹é—´çš„æ¥è§¦ (å¦‚Appleä¸Banana)
        - æœºå™¨äººåº•åº§ä¸åœ°é¢çš„æ¥è§¦
        - å…¶ä»–éæœºå™¨äººç›¸å…³çš„æ¥è§¦
        """
        if self.mj_model is None or self.mj_data is None:
            return False
            
        # è®¾ç½®å…³èŠ‚ä½ç½®å¹¶æ›´æ–°ä»¿çœŸ
        self.mj_data.qpos[:6] = q
        mujoco.mj_forward(self.mj_model, self.mj_data)
        
        # å¦‚æœæ²¡æœ‰ä»»ä½•æ¥è§¦,åˆ™æ²¡æœ‰ç¢°æ’
        if self.mj_data.ncon == 0:
            return False
            
        # éœ€è¦å¿½ç•¥çš„geom (åœ°é¢ã€åæ ‡è½´ã€åœºæ™¯ç‰©ä½“)
        ignored_geom_names = {
            'floor', 'x-aixs', 'y-aixs', 'z-aixs',  # åœ°é¢å’Œåæ ‡è½´
            'Apple', 'Banana', 'mocap',              # åœºæ™¯ç‰©ä½“
            'zone_pickup', 'zone_drop',              # åŒºåŸŸæ ‡è®°
            'table1', 'table2', 'simple_table',      # æ¡Œå­
            'obstacle_box_1', 'obstacle_sphere_1',   # éšœç¢ç‰©
            'obstacle_sphere_2', 'obstacle_sphere_3',
        }
        
        ignored_geom_ids = set()
        for i in range(self.mj_model.ngeom):
            name = mujoco.mj_id2name(self.mj_model, mujoco.mjtObj.mjOBJ_GEOM, i)
            if name and name in ignored_geom_names:
                ignored_geom_ids.add(i)
                
        # æ£€æŸ¥æ¯ä¸ªæ¥è§¦ç‚¹
        for i in range(self.mj_data.ncon):
            contact = self.mj_data.contact[i]
            geom1 = contact.geom1
            geom2 = contact.geom2
            
            # å¦‚æœä»»ä¸€å‡ ä½•ä½“åœ¨å¿½ç•¥åˆ—è¡¨ä¸­,è·³è¿‡
            if geom1 in ignored_geom_ids or geom2 in ignored_geom_ids:
                continue
                
            # åˆ°è¿™é‡Œè¯´æ˜æ˜¯æœºå™¨äººè‡ªç¢°æ’
            # æ£€æŸ¥ç©¿é€æ·±åº¦
            if contact.dist > -0.01:  # å¾ˆå°çš„ç©¿é€,å¿½ç•¥
                continue
            # æ˜¾è‘—ç©¿é€æ‰ç®—ç¢°æ’
            return True
                
        return False
        
    def _build_observation(
        self,
        q_current: np.ndarray,
        q_goal: np.ndarray,
        qd_current: np.ndarray,
        prev_action: np.ndarray,
    ) -> np.ndarray:
        """æ„å»ºè§‚å¯Ÿå‘é‡"""
        ee_pos = self._get_ee_position(q_current)
        ee_goal_pos = self._get_ee_position(q_goal)
        
        obs = np.concatenate([
            q_current,       # 6
            qd_current,      # 6
            q_goal,          # 6
            ee_pos,          # 3
            ee_goal_pos,     # 3
            prev_action,     # 6
        ])
        
        return obs.astype(np.float32)
        
    def plan(
        self,
        q_start: np.ndarray,
        q_goal: np.ndarray,
        validate_collision: bool = True,
        verbose: bool = True,
    ) -> Optional[np.ndarray]:
        """
        ä½¿ç”¨RLç­–ç•¥ç”Ÿæˆè½¨è¿¹
        
        Args:
            q_start: èµ·å§‹å…³èŠ‚é…ç½® (6,)
            q_goal: ç›®æ ‡å…³èŠ‚é…ç½® (6,)
            validate_collision: æ˜¯å¦éªŒè¯ç¢°æ’
            verbose: æ˜¯å¦æ‰“å°ä¿¡æ¯
            
        Returns:
            è½¨è¿¹æ•°ç»„ (n_joints, n_steps) æˆ– None (å¤±è´¥æ—¶)
        """
        if not self.is_ready():
            if verbose:
                print("âŒ RL policy not loaded")
            return None
            
        if verbose:
            print("\nğŸ¤– RL Planner: Generating trajectory...")
            print(f"   Start: {q_start[:3]}...")
            print(f"   Goal:  {q_goal[:3]}...")
            
        # åˆå§‹åŒ–çŠ¶æ€
        q_current = q_start.copy()
        qd_current = np.zeros(6)
        prev_action = np.zeros(6)
        
        trajectory = [q_start.copy()]
        
        for step in range(self.max_steps):
            # æ„å»ºè§‚å¯Ÿ
            obs = self._build_observation(q_current, q_goal, qd_current, prev_action)
            
            # è·å–åŠ¨ä½œ
            action, _ = self.policy.predict(obs, deterministic=True)
            
            # ç¼©æ”¾åŠ¨ä½œ - å¿…é¡»ä¸RLPathEnv.step()ä¸­çš„action_scaleä¸€è‡´
            action_scale = 0.1  # æ¯æ­¥æœ€å¤§ç§»åŠ¨0.1 rad
            scaled_action = action * action_scale
            
            # æ›´æ–°å…³èŠ‚ä½ç½®
            q_new = q_current + scaled_action
            q_new = np.clip(q_new, self.joint_limits_low, self.joint_limits_high)
            
            # ç¢°æ’æ£€æµ‹
            if validate_collision and self._check_collision(q_new):
                if verbose:
                    print(f"âš ï¸ Collision detected at step {step}")
                return None
                
            # æ›´æ–°çŠ¶æ€
            qd_current = (q_new - q_current) / self.dt
            q_current = q_new
            prev_action = action.copy()
            
            trajectory.append(q_current.copy())
            
            # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ç›®æ ‡
            dist_to_goal = np.linalg.norm(q_current - q_goal)
            if dist_to_goal < self.success_threshold:
                if verbose:
                    print(f"âœ… Goal reached in {step + 1} steps!")
                break
                
        # æ£€æŸ¥æœ€ç»ˆæ˜¯å¦æˆåŠŸ
        final_dist = np.linalg.norm(trajectory[-1] - q_goal)
        if final_dist > self.success_threshold:
            if verbose:
                print(f"âŒ Failed to reach goal. Final distance: {final_dist:.4f}")
            return None
            
        # è½¬æ¢ä¸ºè½¨è¿¹æ ¼å¼ (n_joints, n_steps)
        trajectory = np.array(trajectory).T
        
        if verbose:
            print(f"   Trajectory length: {trajectory.shape[1]} points")
            
        return trajectory
        
    def plan_with_smoothing(
        self,
        q_start: np.ndarray,
        q_goal: np.ndarray,
        smoothing_factor: float = 0.1,
        **kwargs
    ) -> Optional[np.ndarray]:
        """
        ç”Ÿæˆè½¨è¿¹å¹¶è¿›è¡Œå¹³æ»‘å¤„ç†
        
        Args:
            q_start: èµ·å§‹é…ç½®
            q_goal: ç›®æ ‡é…ç½®
            smoothing_factor: å¹³æ»‘å› å­ (0-1, è¶Šå¤§è¶Šå¹³æ»‘)
            **kwargs: ä¼ é€’ç»™plan()çš„å…¶ä»–å‚æ•°
            
        Returns:
            å¹³æ»‘åçš„è½¨è¿¹
        """
        trajectory = self.plan(q_start, q_goal, **kwargs)
        
        if trajectory is None:
            return None
            
        # ç®€å•çš„ç§»åŠ¨å¹³å‡å¹³æ»‘
        if smoothing_factor > 0 and trajectory.shape[1] > 5:
            window_size = max(3, int(trajectory.shape[1] * smoothing_factor))
            if window_size % 2 == 0:
                window_size += 1
                
            smoothed = np.zeros_like(trajectory)
            half_window = window_size // 2
            
            for i in range(trajectory.shape[1]):
                start_idx = max(0, i - half_window)
                end_idx = min(trajectory.shape[1], i + half_window + 1)
                smoothed[:, i] = trajectory[:, start_idx:end_idx].mean(axis=1)
                
            # ä¿æŒèµ·ç‚¹å’Œç»ˆç‚¹ä¸å˜
            smoothed[:, 0] = q_start
            smoothed[:, -1] = q_goal
            
            return smoothed
            
        return trajectory


def load_rl_planner(
    model_path: str,
    pin_model: Any = None,
    pin_data: Any = None,
    mj_model: Any = None,
    mj_data: Any = None,
) -> RLPlanner:
    """
    åŠ è½½RLè§„åˆ’å™¨
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        pin_model: Pinocchioæ¨¡å‹
        pin_data: Pinocchioæ•°æ®
        mj_model: MuJoCoæ¨¡å‹
        mj_data: MuJoCoæ•°æ®
        
    Returns:
        RLPlannerå®ä¾‹
    """
    return RLPlanner(
        model_path=model_path,
        pin_model=pin_model,
        pin_data=pin_data,
        mj_model=mj_model,
        mj_data=mj_data,
    )


def get_rl_traj(
    env,
    q_start: np.ndarray,
    q_goal: np.ndarray,
    rl_planner: Optional[RLPlanner] = None,
    model_path: Optional[str] = None,
) -> Optional[np.ndarray]:
    """
    ä½¿ç”¨RLè§„åˆ’å™¨è·å–è½¨è¿¹ (å…¼å®¹ç°æœ‰æ¥å£)
    
    Args:
        env: ç¯å¢ƒå¯¹è±¡ (åŒ…å«æ¨¡å‹ä¿¡æ¯)
        q_start: èµ·å§‹é…ç½®
        q_goal: ç›®æ ‡é…ç½®
        rl_planner: RLè§„åˆ’å™¨å®ä¾‹ (å¯é€‰)
        model_path: æ¨¡å‹è·¯å¾„ (å¦‚æœæœªæä¾›è§„åˆ’å™¨)
        
    Returns:
        è½¨è¿¹ (n_joints, n_steps) æˆ– None
    """
    # ç¡®ä¿æœ‰è§„åˆ’å™¨
    if rl_planner is None:
        if model_path is None:
            # ä½¿ç”¨é»˜è®¤æ¨¡å‹è·¯å¾„
            root_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            model_path = os.path.join(
                root_dir, 
                "manipulator_grasp/rl_path_planner/models/best_model.zip"
            )
            
        rl_planner = load_rl_planner(
            model_path=model_path,
            pin_model=getattr(env, 'model_roboplan', None),
            pin_data=getattr(env, 'data_roboplan', None),
            mj_model=getattr(env, 'model', None),
            mj_data=getattr(env, 'data', None),
        )
        
    if not rl_planner.is_ready():
        print("âŒ RL planner not ready. Please train a model first.")
        return None
        
    # ç”Ÿæˆè½¨è¿¹
    trajectory = rl_planner.plan_with_smoothing(
        q_start[:6],  # åªå–å‰6ä¸ªå…³èŠ‚
        q_goal[:6],
        smoothing_factor=0.1,
    )
    
    if trajectory is not None:
        # æ‰©å±•åˆ°å®Œæ•´å…³èŠ‚ç©ºé—´ (åŒ…æ‹¬å¤¹çˆª)
        full_trajectory = np.zeros((q_start.shape[0], trajectory.shape[1]))
        full_trajectory[:6, :] = trajectory
        
        # å¤¹çˆªä¿æŒä¸å˜
        for i in range(6, q_start.shape[0]):
            full_trajectory[i, :] = q_start[i]
            
        return full_trajectory
        
    return None


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("Testing RL Planner...")
    
    # ä½¿ç”¨é»˜è®¤æ¨¡å‹è·¯å¾„
    root_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    model_path = os.path.join(root_dir, "manipulator_grasp/rl_path_planner/models/best_model.zip")
    
    planner = RLPlanner(model_path=model_path)
    
    if planner.is_ready():
        q_start = np.array([0.0, -np.pi/2, 0.0, -np.pi/2, 0.0, 0.0])
        q_goal = np.array([0.5, -0.8, 0.8, -1.5, -1.57, 0.0])
        
        trajectory = planner.plan(q_start, q_goal, validate_collision=False)
        
        if trajectory is not None:
            print(f"Generated trajectory: {trajectory.shape}")
        else:
            print("Failed to generate trajectory")
    else:
        print("Model not found. Please train a model first.")
        print(f"Expected path: {model_path}")
